# ğŸ” EXPLICATION ULTRA-DÃ‰TAILLÃ‰E DU SCRAPING

## ğŸ¯ Votre Question

**"Tu scrapes d'abord tout avant de rechercher des mots-clÃ©s dans ce qui a Ã©tÃ© obtenu, ou bien tu fais comment? Sur quels sites?"**

---

## ğŸ“Š RÃ‰PONSE COURTE

**NON, on ne scrape PAS tout!**

On envoie directement les mots-clÃ©s (ex: "data-science") aux sites web, et ils nous donnent leurs rÃ©sultats de recherche. C'est comme si vous alliez sur Indeed et tapiez "data-science" dans la barre de recherche.

**3 sites web actuellement supportÃ©s**:
1. âœ… **RemoteOK.com** (spÃ©cialisÃ© 100% remote)
2. âœ… **Indeed.fr** (site gÃ©nÃ©raliste franÃ§ais)
3. âœ… **WelcomeToTheJungle.com** (startups franÃ§aises)

---

## ğŸ”¬ EXPLICATION TECHNIQUE DÃ‰TAILLÃ‰E

### Ã‰tape 1: Vous Cliquez "Rechercher"

**Frontend envoie**:
```javascript
GET /api/v1/jobs/search?keyword=data-science&location=Paris&job_type=Stage
```

---

### Ã‰tape 2: Backend Appelle `search_hybrid()`

**Fichier**: `backend/app/services/search_service.py`

```python
async def search_hybrid(db, user_id, keywords, location, job_type, ...):
    # 1. Recherche DB locale
    db_offers = await JobOfferService.search_job_offers(...)
    
    # 2. Scraping Internet (si keywords fourni)
    if enable_scraping and keywords:
        scraping_result = await search_with_scraping(
            keywords=keywords,  # "data-science"
            location=location,   # "Paris"
            job_type=job_type,   # "Stage"
        )
```

---

### Ã‰tape 3: `search_with_scraping()` Lance Le Scraping

**Fichier**: `backend/app/services/search_service.py` ligne 28

```python
async def search_with_scraping(keywords, location, job_type, ...):
    # 1. Appelle le ScrapingService
    raw_results = await scraping_service.scrape_all_platforms(
        keywords=keywords,      # "data-science"
        location=location,      # "Paris"
        limit_per_platform=30   # Max 30 offres par site
    )
    
    # raw_results = {
    #     "remoteok": [offre1, offre2, ...],
    #     "indeed": [offre1, offre2, ...],
    #     "welcometothejungle": [offre1, offre2, ...]
    # }
```

---

### Ã‰tape 4: `scrape_all_platforms()` Scrape Les 3 Sites EN PARALLÃˆLE

**Fichier**: `backend/app/services/scraping_service.py` ligne 226

```python
async def scrape_all_platforms(keywords, location, limit_per_platform):
    """
    Lance le scraping sur les 3 sites EN MÃŠME TEMPS (parallÃ¨le)
    """
    results = {}
    
    # Scraping PARALLÃˆLE (3 sites en mÃªme temps)
    tasks = []
    for platform_name in ["remoteok", "indeed", "welcometothejungle"]:
        task = scrape_platform(
            platform_name,
            keywords="data-science",
            location="Paris",
            limit=30
        )
        tasks.append(task)
    
    # Attendre que les 3 scrapers finissent
    platform_results = await asyncio.gather(*tasks, return_exceptions=True)
    
    return {
        "remoteok": platform_results[0],           # 10 offres
        "indeed": platform_results[1],             # 15 offres
        "welcometothejungle": platform_results[2]  # 8 offres
    }
```

**Important**: Les 3 sites sont scrapÃ©s **en mÃªme temps**, pas l'un aprÃ¨s l'autre!

---

### Ã‰tape 5: Chaque Scraper Fait Sa Recherche

Maintenant, dÃ©taillons **comment chaque site est scrapÃ©**.

---

## ğŸŒ SITE 1: RemoteOK.com

**Fichier**: `backend/app/services/scrapers/remoteok_scraper.py`

### MÃ©thode: API Publique (Pas de Scraping HTML)

RemoteOK a une **API publique gratuite** : `https://remoteok.com/api`

```python
async def scrape(keywords, location, max_results):
    """
    Scraper RemoteOK avec leur API publique
    """
    
    # 1. Appeler l'API publique
    api_url = "https://remoteok.com/api"
    
    # Fetch JSON
    response = await fetch(api_url)
    jobs = await response.json()  # Retourne TOUTES les offres
    
    # 2. Filtrer cÃ´tÃ© client avec les mots-clÃ©s
    filtered_jobs = []
    for job in jobs:
        # Chercher "data-science" dans le titre, tags, description
        if match_keywords(job, keywords="data-science"):
            filtered_jobs.append(job)
        
        if len(filtered_jobs) >= max_results:
            break
    
    # 3. Convertir en format standardisÃ©
    offers = []
    for job in filtered_jobs:
        offers.append({
            "title": job["position"],         # "Data Scientist"
            "company": job["company"],        # "Google"
            "location": job["location"] or "Remote",
            "description": job["description"],
            "url": f"https://remoteok.com/remote-jobs/{job['slug']}",
            "source_platform": "remoteok",
            "job_type": detect_job_type(job["tags"]),  # DÃ©tecte "Stage" depuis tags
            "work_mode": "remote",            # Toujours remote
            "scraped_at": datetime.utcnow()
        })
    
    return offers
```

### Processus RemoteOK:

```
1. GET https://remoteok.com/api
   â†’ Retourne ~500 offres en JSON (TOUTES les offres du site)

2. Filtrage LOCAL (cÃ´tÃ© Python):
   - Pour chaque offre:
     - Est-ce que "data-science" est dans title, tags ou description?
     - Est-ce que "Paris" est dans location? (rarissime car 100% remote)
   
   - Si OUI â†’ Garder
   - Si NON â†’ Ignorer

3. Limiter Ã  30 offres max

4. Retourner les offres filtrÃ©es
```

**Avantages**:
- âœ… TrÃ¨s rapide (1 seul appel HTTP)
- âœ… Pas de dÃ©tection anti-bot
- âœ… DonnÃ©es structurÃ©es (JSON)

**InconvÃ©nients**:
- âš ï¸ On rÃ©cupÃ¨re TOUTES les offres (~500), puis on filtre
- âš ï¸ Pas de recherche cÃ´tÃ© serveur (RemoteOK ne propose pas d'API de recherche)

---

## ğŸŒ SITE 2: Indeed.fr

**Fichier**: `backend/app/services/scrapers/indeed_scraper.py`

### MÃ©thode: Scraping HTML avec Playwright

Indeed n'a **pas d'API publique**, on doit scraper le HTML directement.

```python
async def scrape(keywords, location, job_type, max_results):
    """
    Scraper Indeed.fr en simulant un navigateur
    """
    
    # 1. Construire l'URL de recherche
    search_url = _build_search_url(keywords, location, job_type)
    # RÃ©sultat: "https://fr.indeed.com/jobs?q=data-science&l=Paris&jt=internship"
    
    # 2. Ouvrir un navigateur headless (Playwright)
    await init_browser()  # Lance Chrome en mode invisible
    
    # 3. Naviguer vers l'URL de recherche
    page = await browser.new_page()
    await page.goto(search_url)
    
    # Indeed affiche directement les rÃ©sultats filtrÃ©s!
    # On ne rÃ©cupÃ¨re PAS toutes les offres, juste celles qui matchent.
    
    # 4. Attendre que la page charge
    await page.wait_for_selector(".job_seen_beacon, .jobsearch-ResultsList")
    await sleep(random(2, 4))  # Anti-bot: attente alÃ©atoire
    
    # 5. Extraire les offres de la page HTML
    offers = []
    job_cards = await page.query_selector_all(".job_seen_beacon")
    
    for card in job_cards:
        # Extraire titre
        title_elem = await card.query_selector(".jobTitle span")
        title = await title_elem.inner_text()
        
        # Extraire entreprise
        company_elem = await card.query_selector(".companyName")
        company = await company_elem.inner_text()
        
        # Extraire localisation
        location_elem = await card.query_selector(".companyLocation")
        location = await location_elem.inner_text()
        
        # Extraire URL
        link_elem = await card.query_selector("a[id^='job_']")
        href = await link_elem.get_attribute("href")
        url = f"https://fr.indeed.com{href}"
        
        # CrÃ©er l'offre
        offers.append({
            "title": title,
            "company": company,
            "location": location,
            "description": "",  # Pas dispo sur page rÃ©sultats
            "url": url,
            "source_platform": "indeed",
            "job_type": job_type or "fulltime",
            "work_mode": "onsite",  # Par dÃ©faut
            "scraped_at": datetime.utcnow()
        })
        
        if len(offers) >= max_results:
            break
    
    # 6. Pagination (si besoin)
    if len(offers) < max_results:
        # Cliquer sur "Page suivante"
        next_button = await page.query_selector("a[data-testid='pagination-page-next']")
        if next_button:
            await next_button.click()
            await page.wait_for_selector(".job_seen_beacon")
            # RÃ©pÃ©ter l'extraction...
    
    # 7. Fermer le navigateur
    await browser.close()
    
    return offers
```

### Processus Indeed:

```
1. Construire URL de recherche:
   https://fr.indeed.com/jobs?q=data-science&l=Paris&jt=internship
   
   ParamÃ¨tres:
   - q = mots-clÃ©s ("data-science")
   - l = localisation ("Paris")
   - jt = job type ("internship" = Stage)

2. Indeed fait la recherche cÃ´tÃ© serveur:
   - Indeed cherche "data-science" dans sa base de donnÃ©es
   - Indeed filtre par "Paris"
   - Indeed filtre par "Stage"
   - Indeed retourne une page HTML avec 15 rÃ©sultats

3. On ouvre la page HTML avec Playwright (Chrome headless)

4. On extrait les donnÃ©es HTML:
   - SÃ©lecteurs CSS: ".job_seen_beacon", ".jobTitle", ".companyName"
   - On parse le HTML pour extraire texte

5. On pagine (cliquer "Page suivante") si besoin

6. On retourne les offres extraites
```

**Avantages**:
- âœ… Recherche cÃ´tÃ© serveur (Indeed filtre pour nous)
- âœ… DonnÃ©es dÃ©jÃ  filtrÃ©es par Indeed
- âœ… Pas besoin de tout rÃ©cupÃ©rer

**InconvÃ©nients**:
- âš ï¸ Plus lent (navigateur headless)
- âš ï¸ Risque anti-bot (Playwright simule un vrai navigateur pour contourner)
- âš ï¸ Parsing HTML fragile (si Indeed change le HTML, Ã§a casse)

---

## ğŸŒ SITE 3: WelcomeToTheJungle.com

**Fichier**: `backend/app/services/scrapers/wttj_scraper.py`

### MÃ©thode: Scraping HTML avec Playwright

Similaire Ã  Indeed, mais pour le site franÃ§ais WTTJ.

```python
async def scrape(keywords, location, job_type, max_results):
    """
    Scraper WelcomeToTheJungle.com
    """
    
    # 1. Construire l'URL
    search_url = f"https://www.welcometothejungle.com/fr/jobs?query={keywords}&refinementList[location.name][]={location}"
    # RÃ©sultat: "https://www.welcometothejungle.com/fr/jobs?query=data-science&refinementList[location.name][]=Paris"
    
    # 2. Ouvrir navigateur
    page = await browser.new_page()
    await page.goto(search_url)
    
    # 3. Attendre chargement (WTTJ utilise React/JS)
    await page.wait_for_selector("li[data-testid='job-list-item']", timeout=10000)
    await sleep(random(2, 4))
    
    # 4. Extraire offres
    job_items = await page.query_selector_all("li[data-testid='job-list-item']")
    
    offers = []
    for item in job_items:
        # Titre
        title_elem = await item.query_selector("h3")
        title = await title_elem.inner_text()
        
        # Entreprise
        company_elem = await item.query_selector(".company-name")
        company = await company_elem.inner_text()
        
        # URL
        link = await item.query_selector("a")
        href = await link.get_attribute("href")
        url = f"https://www.welcometothejungle.com{href}"
        
        offers.append({
            "title": title,
            "company": company,
            "location": location or "Paris",
            "url": url,
            "source_platform": "welcometothejungle",
            "scraped_at": datetime.utcnow()
        })
        
        if len(offers) >= max_results:
            break
    
    await browser.close()
    return offers
```

**Processus WTTJ**: Identique Ã  Indeed (recherche cÃ´tÃ© serveur, scraping HTML).

---

## ğŸ”„ Ã‰TAPE 6: DÃ©duplication et Filtrage

Une fois les 3 scrapers terminÃ©s:

```python
# RÃ©sultats bruts
raw_results = {
    "remoteok": [10 offres],
    "indeed": [15 offres],
    "welcometothejungle": [8 offres]
}

# Total: 33 offres brutes

# 1. Aplatir la liste
all_offers = []
for platform, offers in raw_results.items():
    all_offers.extend(offers)

# all_offers = [33 offres]

# 2. DÃ©duplication (enlever doublons)
deduplicated = []
seen_urls = set()
seen_signatures = set()

for offer in all_offers:
    # Doublon par URL?
    if offer["url"] in seen_urls:
        continue  # Ignorer
    
    # Doublon par titre+entreprise?
    signature = f"{offer['title']}|{offer['company']}"
    if signature in seen_signatures:
        continue  # Ignorer
    
    # C'est unique, on garde
    deduplicated.append(offer)
    seen_urls.add(offer["url"])
    seen_signatures.add(signature)

# deduplicated = [30 offres] (3 doublons enlevÃ©s)

# 3. Filtrage par job_type (si spÃ©cifiÃ©)
if job_type == "Stage":
    filtered = []
    for offer in deduplicated:
        # Est-ce un stage?
        if is_internship(offer):
            filtered.append(offer)

# filtered = [12 offres] (que des stages)

# 4. Sauvegarde en DB
for offer in filtered:
    db.insert(JobOffer(**offer, user_id=current_user.id))

# 5. Retour au frontend
return filtered  # 12 offres
```

---

## ğŸ“Š RÃ‰SUMÃ‰ DU PROCESSUS COMPLET

### Vous cherchez: "data-science + Paris + Stage"

```
1. Frontend â†’ Backend: GET /api/v1/jobs/search?keyword=data-science&location=Paris&job_type=Stage

2. Backend lance 3 scrapers EN PARALLÃˆLE:

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                  ScrapingService                        â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚                                                         â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
   â”‚  â”‚ RemoteOK    â”‚  â”‚ Indeed.fr   â”‚  â”‚ WTTJ.com     â”‚  â”‚
   â”‚  â”‚             â”‚  â”‚             â”‚  â”‚              â”‚  â”‚
   â”‚  â”‚ API: GET    â”‚  â”‚ URL: https  â”‚  â”‚ URL: https   â”‚  â”‚
   â”‚  â”‚ /api        â”‚  â”‚ //fr.indeed â”‚  â”‚ //wttj/jobs  â”‚  â”‚
   â”‚  â”‚             â”‚  â”‚ .com/jobs?q â”‚  â”‚ ?query=data  â”‚  â”‚
   â”‚  â”‚ Filtre:     â”‚  â”‚ =data-scien â”‚  â”‚ -science     â”‚  â”‚
   â”‚  â”‚ "data-sci"  â”‚  â”‚ ce&l=Paris  â”‚  â”‚ &location=   â”‚  â”‚
   â”‚  â”‚ in tags     â”‚  â”‚ &jt=intern  â”‚  â”‚ Paris        â”‚  â”‚
   â”‚  â”‚             â”‚  â”‚             â”‚  â”‚              â”‚  â”‚
   â”‚  â”‚ RÃ©sultat:   â”‚  â”‚ RÃ©sultat:   â”‚  â”‚ RÃ©sultat:    â”‚  â”‚
   â”‚  â”‚ 10 offres   â”‚  â”‚ 15 offres   â”‚  â”‚ 8 offres     â”‚  â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
   â”‚         â†“                 â†“                 â†“         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                    [33 offres brutes]
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  DÃ©duplication   â”‚
                    â”‚  - Par URL       â”‚
                    â”‚  - Par titre+cie â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                    [30 offres uniques]
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Filtrage Stage  â”‚
                    â”‚  - Cherche "stag"â”‚
                    â”‚    "intern" dans â”‚
                    â”‚    titre/type    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                    [12 offres de stage]
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Sauvegarde DB   â”‚
                    â”‚  user_id=vous    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                    [Retour Frontend]
                    12 offres affichÃ©es
```

---

## ğŸ¯ RÃ‰PONSES Ã€ VOS QUESTIONS

### Q1: "Tu scrapes tout avant de rechercher?"

**NON!** 

- âœ… **RemoteOK**: On rÃ©cupÃ¨re toutes les ~500 offres de l'API, PUIS on filtre localement
- âœ… **Indeed**: On envoie directement "data-science + Paris" dans l'URL, Indeed filtre cÃ´tÃ© serveur
- âœ… **WTTJ**: Pareil, on envoie les mots-clÃ©s, WTTJ filtre cÃ´tÃ© serveur

**Seul RemoteOK rÃ©cupÃ¨re tout** (car leur API ne supporte pas la recherche).

---

### Q2: "Sur quels sites?"

**3 sites actuellement**:

1. âœ… **RemoteOK.com** 
   - SpÃ©cialitÃ©: Jobs 100% remote
   - MÃ©thode: API publique JSON
   - Vitesse: TrÃ¨s rapide (1 requÃªte HTTP)

2. âœ… **Indeed.fr**
   - SpÃ©cialitÃ©: Site gÃ©nÃ©raliste franÃ§ais
   - MÃ©thode: Scraping HTML avec Playwright
   - Vitesse: Moyen (3-5 secondes par page)

3. âœ… **WelcomeToTheJungle.com**
   - SpÃ©cialitÃ©: Startups et scale-ups franÃ§aises
   - MÃ©thode: Scraping HTML avec Playwright
   - Vitesse: Moyen (3-5 secondes)

**Total temps**: 10-30 secondes pour scraper les 3 sites en parallÃ¨le.

---

### Q3: "Comment tu filtres?"

**Filtrage en 2 Ã©tapes**:

1. **Filtrage par site** (pendant le scraping):
   - RemoteOK: Cherche "data-science" dans title, tags, description
   - Indeed: Indeed filtre cÃ´tÃ© serveur (URL avec ?q=data-science)
   - WTTJ: WTTJ filtre cÃ´tÃ© serveur

2. **Filtrage aprÃ¨s scraping** (backend Python):
   ```python
   # Filtre job_type="Stage"
   for offer in all_offers:
       title_lower = offer["title"].lower()
       job_type_lower = offer.get("job_type", "").lower()
       
       # DÃ©tecte "stage" ou "internship"
       if "stage" in title_lower or "intern" in title_lower or "internship" in job_type_lower:
           # C'est un stage!
           filtered_offers.append(offer)
   ```

---

## ğŸ”§ CONFIGURATION DES PLATEFORMES

**Fichier**: `backend/app/platforms_config/platforms.py`

```python
PLATFORMS = {
    "remoteok": {
        "enabled": True,
        "priority": 1,
        "rate_limit": 500,  # 500 requÃªtes/heure
        "scraper_class": "RemoteOKScraper"
    },
    "indeed": {
        "enabled": True,
        "priority": 2,
        "rate_limit": 100,  # 100 requÃªtes/heure
        "scraper_class": "IndeedScraper"
    },
    "welcometothejungle": {
        "enabled": True,
        "priority": 3,
        "rate_limit": 200,
        "scraper_class": "WTTJScraper"
    }
}
```

---

## ğŸš€ EN RÃ‰SUMÃ‰

| Site | MÃ©thode | Recherche | Vitesse | Limites |
|------|---------|-----------|---------|---------|
| RemoteOK | API JSON | Local (Python) | âš¡ TrÃ¨s rapide | 500 req/h |
| Indeed | HTML Scraping | Serveur (URL) | ğŸ¢ Moyen | 100 req/h, anti-bot |
| WTTJ | HTML Scraping | Serveur (URL) | ğŸ¢ Moyen | 200 req/h |

**StratÃ©gie**: 
- On envoie les mots-clÃ©s directement aux sites
- Les sites filtrent (sauf RemoteOK)
- On rÃ©cupÃ¨re les rÃ©sultats filtrÃ©s
- On dÃ©duplique et re-filtre si besoin

**Pas de "scrape tout puis filtre"** sauf pour RemoteOK!

---

**Date**: 2026-01-31  
**Fichiers analysÃ©s**: 
- `backend/app/services/scraping_service.py`
- `backend/app/services/scrapers/remoteok_scraper.py`
- `backend/app/services/scrapers/indeed_scraper.py`
- `backend/app/services/scrapers/wttj_scraper.py`
