# üîç Guide : Veille d'Entreprise (Company Watch)

## üìã Qu'est-ce que c'est ?

La **veille d'entreprise** permet de surveiller automatiquement les offres d'emploi publi√©es par des entreprises sp√©cifiques qui vous int√©ressent.

## üéØ Fonctionnement

### 1. **Ajouter une entreprise √† surveiller**
```
Page : /companies/watch
Action : Ajouter une entreprise (ex: "Google", "Microsoft")
```

Vous pouvez fournir :
- ‚úÖ **Nom de l'entreprise** (obligatoire)
- ‚öôÔ∏è **Seuil d'alerte** : Score minimum de compatibilit√© (d√©faut: 70%)
- üîó **URLs optionnelles** :
  - LinkedIn (page entreprise)
  - Careers page (page carri√®res)
  - Indeed, Welcome to the Jungle, etc.

### 2. **Scraping automatique**

Le syst√®me scrape **automatiquement** :
- ‚è∞ **Fr√©quence** : Toutes les 24h par d√©faut (configurable)
- ü§ñ **Celery** : Task asynchrone en arri√®re-plan
- üìä **Sources** : LinkedIn, page carri√®res, Indeed, WTTJ

### 3. **Scoring intelligent**

Pour chaque offre trouv√©e :
- üß† **Analyse IA** : Calcul de compatibilit√© avec votre profil
- üéØ **Score** : 0-100% bas√© sur :
  - Vos comp√©tences vs. offre
  - Votre exp√©rience vs. exigences
  - Embedding s√©mantique (similarit√©)

### 4. **Alertes automatiques**

Si `score >= seuil d'alerte` :
- üìß **Email** : Notification automatique (si configur√©)
- üîî **Dashboard** : Badge sur l'interface
- üìå **Liste prioritaire** : Offres tri√©es par score

### 5. **Mutualisation** (Optimisation)

Si plusieurs utilisateurs surveillent la m√™me entreprise :
- ‚úÖ **1 seul scraping** pour tous les users
- üöÄ **√âconomie de ressources**
- üìä **Compteur "watchers"** : voir combien d'users suivent l'entreprise

## üõ†Ô∏è Impl√©mentation Technique

### Backend

#### **Routes API** (`app/api/routes/company_watch.py`)
- ‚úÖ `POST /api/v1/watch/company` : Ajouter une veille
- ‚úÖ `GET /api/v1/watch/companies` : Lister mes veilles
- ‚úÖ `GET /api/v1/watch/company/{slug}/offers` : Offres trouv√©es
- ‚úÖ `DELETE /api/v1/watch/{watch_id}` : Supprimer veille

#### **Service** (`app/services/company_watch_service.py`)
- ‚úÖ Gestion des veilles (CRUD)
- ‚úÖ Cr√©ation de slug unique : `slugify(company_name)`
- ‚úÖ Mutualisation : r√©utilise `watched_companies` si existe

#### **Celery Tasks** (`app/tasks/company_watch_tasks.py`)
- ‚úÖ `scrape_watched_companies()` : Task p√©riodique (toutes les 24h)
- ‚úÖ Scraping parall√®le (asyncio)
- ‚úÖ Scoring automatique des offres

#### **Mod√®les** (`app/models/watched_company.py`)
```python
watched_companies:
  - id, company_name, company_slug
  - linkedin_url, careers_url, indeed_url, wttj_url
  - last_scraped_at, scraping_frequency
  - total_watchers, total_offers_found

user_company_watches (table pivot):
  - user_id, watched_company_id
  - profile_id (pour scoring)
  - alert_threshold
```

### Frontend

#### **Page** (`frontend/src/app/companies/watch/page.tsx`)
- ‚úÖ Formulaire d'ajout d'entreprise
- ‚úÖ Liste des veilles actives
- ‚úÖ Affichage des offres trouv√©es

#### **Service** (`frontend/src/lib/companiesService.ts`)
- ‚úÖ `addCompanyWatch()`
- ‚úÖ `getWatchedCompanies()`
- ‚úÖ `getCompanyOffers()`
- ‚úÖ `deleteWatch()`

## ‚úÖ √âtat d'Impl√©mentation

### ‚úÖ **COMPLET**
- [x] Mod√®les de donn√©es
- [x] API Backend (CRUD)
- [x] Service de scraping
- [x] Celery task automatique
- [x] Scoring IA
- [x] Frontend UI
- [x] Mutualisation

### ‚ö†Ô∏è **√Ä AM√âLIORER**
- [ ] **Notifications email** : Configurer SMTP
- [ ] **Tests E2E** : V√©rifier scraping r√©el
- [ ] **UI polish** : Graphiques de tendances

### ‚ùå **MANQUANT**
- [ ] **Webhooks** : Notifications temps r√©el
- [ ] **Fr√©quence personnalisable** : UI pour changer fr√©quence de scraping
- [ ] **Export CSV** : T√©l√©charger toutes les offres trouv√©es

## üöÄ Comment Tester

### 1. Ajouter une veille
```bash
# Via UI
http://localhost:3000/companies/watch

# Via API
curl -X POST http://localhost:8000/api/v1/watch/company \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "company_name": "OpenAI",
    "alert_threshold": 75
  }'
```

### 2. D√©clencher scraping manuellement
```bash
# Se connecter au container backend
docker compose -f docker-compose.prod.yml exec backend bash

# Lancer le scraping
python -c "from app.tasks.company_watch_tasks import scrape_watched_companies; import asyncio; asyncio.run(scrape_watched_companies())"
```

### 3. V√©rifier les offres trouv√©es
```
GET /api/v1/watch/company/{company-slug}/offers
```

## üìä Exemple de Workflow

```
1. User ajoute "Google" √† surveiller avec seuil 80%
   ‚îî‚îÄ> watched_companies cr√©√©e (slug: "google")
   ‚îî‚îÄ> user_company_watches cr√©√©e (user_id, watched_company_id)

2. Celery scrape toutes les 24h
   ‚îî‚îÄ> Scraping LinkedIn + Careers page
   ‚îî‚îÄ> 15 offres trouv√©es
   ‚îî‚îÄ> Pour chaque offre:
       ‚îú‚îÄ> Calcul score vs profil user (0-100%)
       ‚îú‚îÄ> Si score >= 80% ‚Üí Alerte email
       ‚îî‚îÄ> Sauvegarde dans job_offers

3. User consulte /companies/watch
   ‚îî‚îÄ> Voit "Google" avec badge "5 nouvelles offres (score >= 80%)"
   ‚îî‚îÄ> Clique ‚Üí Liste des 5 offres compatibles
   ‚îî‚îÄ> G√©n√®re CV + LM en 1 clic
```

## üîß Configuration

### Environnement
```env
# Fr√©quence scraping (heures)
COMPANY_WATCH_SCRAPING_FREQUENCY=24

# Score minimum par d√©faut
DEFAULT_ALERT_THRESHOLD=70

# Email (optionnel)
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=your-email@gmail.com
SMTP_PASSWORD=your-app-password
```

### Celery Beat Schedule
```python
# backend/app/celery_app.py
CELERY_BEAT_SCHEDULE = {
    'scrape-watched-companies': {
        'task': 'app.tasks.company_watch_tasks.scrape_watched_companies',
        'schedule': crontab(hour=2, minute=0),  # Tous les jours √† 2h du matin
    },
}
```

## üéì Conclusion

La veille d'entreprise est **100% fonctionnelle** c√¥t√© backend et frontend. Le seul √©l√©ment manquant pour une exp√©rience compl√®te est la **configuration SMTP** pour les notifications email automatiques.

**Next Steps** :
1. Configurer SMTP dans `.env.prod`
2. Tester le scraping en production
3. Ajouter UI pour ajuster fr√©quence de scraping par entreprise
