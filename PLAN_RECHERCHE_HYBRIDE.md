# ‚úÖ RECHERCHE HYBRIDE IMPL√âMENT√âE

## üéØ Ce Qui a √ât√© Fait

### Nouvelle Fonctionnalit√©: Recherche Hybride (DB + Scraping)

L'endpoint `/api/v1/jobs/search` fait maintenant **DEUX choses en parall√®le**:

1. ‚úÖ **Recherche dans la base de donn√©es locale** (vos offres sauvegard√©es)
2. ‚úÖ **Scraping en temps r√©el sur Internet** (RemoteOK, Indeed, WTTJ)
3. ‚úÖ **Combine les r√©sultats** et d√©duplique
4. ‚úÖ **Retourne la liste unifi√©e**

---

## üìù Code Modifi√©

### 1. `backend/app/services/search_service.py`

**Nouvelle m√©thode ajout√©e**: `search_hybrid()`

```python
async def search_hybrid(
    self,
    db: AsyncSession,
    user_id: str,
    keywords: Optional[str] = None,
    location: Optional[str] = None,
    job_type: Optional[str] = None,
    company: Optional[str] = None,
    enable_scraping: bool = True,
    limit: int = 50
) -> Dict:
    """
    Recherche hybride: DB locale + Scraping Internet
    
    1. Cherche dans la DB de l'utilisateur
    2. Si scraping activ√©, lance le scraping
    3. Combine et d√©duplique les r√©sultats
    4. Retourne la liste unifi√©e
    """
```

**Logique**:
```
1. Recherche DB ‚Üí trouve X offres locales
2. Scraping Internet (si keywords fourni) ‚Üí trouve Y offres
3. Combine: X + Y offres
4. D√©duplique (par URL et titre+entreprise)
5. Retourne offres uniques
```

---

### 2. `backend/app/api/job_offer.py`

**Endpoint modifi√©**: `GET /api/v1/jobs/search`

**Nouveau param√®tre**: `enable_scraping` (d√©faut: `True`)

**Avant**:
```python
# Cherchait UNIQUEMENT dans la DB
offers = await JobOfferService.search_job_offers(db, user_id, ...)
return offers
```

**Maintenant**:
```python
# Recherche HYBRIDE (DB + Scraping)
result = await search_service.search_hybrid(
    db=db,
    user_id=str(current_user.id),
    keywords=keyword,
    location=location,
    job_type=job_type,
    enable_scraping=enable_scraping,
    limit=limit
)
return result["offers"]
```

---

## üîç Comment √áa Marche

### Sc√©nario 1: Recherche "data-science + Paris + Stage"

```
Frontend envoie:
GET /api/v1/jobs/search?keyword=data-science&location=Paris&job_type=Stage

Backend fait:

1. Recherche DB locale
   - Cherche dans vos offres sauvegard√©es
   - R√©sultat: 0 offre (vous n'en avez pas encore)

2. Scraping Internet
   - RemoteOK scraper ‚Üí 5 offres "data-science"
   - Indeed scraper ‚Üí 8 offres "data-science Paris"
   - WTTJ scraper ‚Üí 3 offres "data science internship"
   - Total brut: 16 offres

3. Filtrage
   - Filtre par "Stage" (job_type)
   - Filtre par "Paris" (location)
   - R√©sultat: 7 offres matchent

4. D√©duplication
   - Enl√®ve les doublons (m√™me URL ou m√™me titre+entreprise)
   - R√©sultat final: 5 offres uniques

5. Sauvegarde en DB
   - Les 5 offres sont sauvegard√©es dans VOTRE compte
   - Prochaine recherche, elles seront dans la DB

Backend retourne: 5 offres
```

---

### Sc√©nario 2: Recherche "Python + Paris" (2√®me fois)

```
1. Recherche DB locale
   - Trouve 2 offres sauvegard√©es lors d'une recherche pr√©c√©dente

2. Scraping Internet
   - RemoteOK ‚Üí 10 nouvelles offres
   - Indeed ‚Üí 15 nouvelles offres

3. Combine
   - 2 (DB) + 25 (scraping) = 27 offres

4. D√©duplique
   - Les 2 offres de la DB sont d√©j√† dans le scraping ‚Üí d√©dupliqu√©es
   - R√©sultat: 23 offres uniques

Backend retourne: 23 offres
```

---

## ÔøΩÔøΩÔ∏è Param√®tres de l'Endpoint

### GET /api/v1/jobs/search

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `keyword` | string | null | Mots-cl√©s (ex: "data-science", "Python") |
| `location` | string | null | Localisation (ex: "Paris", "Remote") |
| `job_type` | string | null | Type de contrat (Stage, CDI, CDD, etc.) |
| `company_name` | string | null | Nom d'entreprise sp√©cifique |
| `enable_scraping` | bool | **true** | Active/d√©sactive le scraping |
| `limit` | int | 50 | Nombre max de r√©sultats |

---

## ‚úÖ Avantages

### 1. **R√©sultats Imm√©diats**
- Vous voyez vos offres sauvegard√©es instantan√©ment
- Puis les nouvelles offres arrivent du scraping

### 2. **Pas de Perte**
- Toutes les offres trouv√©es sont sauvegard√©es dans VOTRE DB
- Vous pouvez les retrouver plus tard sans re-scraper

### 3. **D√©duplication Intelligente**
- Pas de doublons entre DB et scraping
- M√™me offre sur RemoteOK et Indeed ‚Üí compt√©e 1 fois

### 4. **D√©sactivable**
- Vous pouvez faire `enable_scraping=false` pour rechercher UNIQUEMENT dans votre DB

---

## ‚ö†Ô∏è Limitations Actuelles

### 1. **Synchrone** (pas async)
- Le scraping prend 10-30 secondes
- Vous devez attendre la fin
- **Solution future**: Celery + polling async

### 2. **Pas de Feedback Progressif**
- Vous ne voyez pas "5 offres... 10 offres... 20 offres..."
- Vous voyez juste un spinner qui tourne
- **Solution future**: WebSockets ou polling

### 3. **Celery Worker Crash**
- Le worker Celery ne fonctionne pas (manque pgvector)
- Impossible d'utiliser le mode async
- **Solution**: Fixer Celery (√©tape suivante)

---

## üß™ Test de la Recherche Hybride

### √âtape 1: Tester Sans Scraping (DB uniquement)

```bash
curl -X GET "http://localhost:8000/api/v1/jobs/search?keyword=Python&enable_scraping=false" \
  -H "Authorization: Bearer YOUR_TOKEN"
```

**R√©sultat attendu**: 0 offres (DB vide)

---

### √âtape 2: Tester Avec Scraping (Hybride)

**Frontend**:
1. Allez sur http://localhost:3000/jobs
2. Cherchez: "data-science + Paris + Stage"
3. Cliquez "Rechercher"
4. **Attendez 10-30 secondes** (scraping en cours)
5. R√©sultat: Offres fra√Æches d'Internet s'affichent

**Backend logs** (dans Docker):
```
[API] Recherche hybride lanc√©e par kenfackfranck08@gmail.com
[API] Params: keyword=data-science, location=Paris, job_type=Stage, scraping=True
[SearchHybrid] Recherche DB pour user <uuid>
[SearchHybrid] 0 offres trouv√©es en DB
[SearchHybrid] Lancement scraping pour 'data-science'
[SearchService] D√©but scraping: keywords=data-science, location=Paris
[SearchService] 15 offres brutes r√©cup√©r√©es
[SearchService] 12 offres apr√®s d√©duplication
[SearchService] 7 offres apr√®s filtrage
[SearchService] 7 offres sauvegard√©es en DB
[SearchHybrid] 7 offres scrap√©es
[SearchHybrid] 7 offres avant d√©duplication
[SearchHybrid] 7 offres apr√®s d√©duplication
[API] R√©sultats: 7 offres (0 DB + 7 scraping)
```

---

### √âtape 3: Tester la Persistance

**Refaire la m√™me recherche imm√©diatement**:

```
1. Cherchez √† nouveau "data-science + Paris + Stage"
2. Cette fois, r√©sultats plus rapides (DB + scraping)
3. R√©sultat: Vous voyez les 7 offres de la DB + nouvelles du scraping
```

**Backend logs**:
```
[SearchHybrid] 7 offres trouv√©es en DB  ‚Üê Les offres sauvegard√©es
[SearchHybrid] 5 offres scrap√©es       ‚Üê Nouvelles offres
[SearchHybrid] 12 offres avant d√©duplication
[SearchHybrid] 10 offres apr√®s d√©duplication  ‚Üê 2 doublons enlev√©s
[API] R√©sultats: 10 offres (7 DB + 3 scraping nouveaux)
```

---

## üìä Comparaison Avant/Apr√®s

| Aspect | AVANT ‚ùå | MAINTENANT ‚úÖ |
|--------|---------|---------------|
| Recherche | DB uniquement | DB + Internet |
| R√©sultats pour "data-science" | 0 offres | 5-15 offres r√©elles |
| Scraping | ‚ùå Pas impl√©ment√© | ‚úÖ Actif |
| Persistance | ‚ùå Rien sauvegard√© | ‚úÖ Offres sauvegard√©es |
| D√©duplication | ‚ùå Non | ‚úÖ Oui |
| Temps de r√©ponse | < 1s (DB vide) | 10-30s (scraping) |

---

## üöÄ PROCHAINE √âTAPE: Option B (Celery + Async)

### Objectif

Remplacer la recherche **synchrone** par **asynchrone avec feedback progressif**:

```
1. Frontend envoie requ√™te
2. Backend r√©pond imm√©diatement: "task_id = 123abc"
3. Frontend poll toutes les 2 secondes:
   - "En cours... 5 offres trouv√©es"
   - "En cours... 12 offres trouv√©es"
   - "Termin√©! 18 offres trouv√©es"
4. Meilleure UX: l'utilisateur voit la progression
```

### Plan

1. ‚úÖ **Fixer Celery Worker**
   ```bash
   echo "pgvector==0.2.4" >> backend/requirements.txt
   docker compose down
   docker compose up -d --build
   ```

2. ‚úÖ **Cr√©er endpoint async**
   - POST `/api/v1/jobs/search/async` ‚Üí retourne `task_id`
   - GET `/api/v1/jobs/search/status/{task_id}` ‚Üí retourne √©tat

3. ‚úÖ **Modifier frontend**
   - Utiliser `searchJobsAsync()` au lieu de `searchJobOffers()`
   - Polling toutes les 2s
   - Afficher messages progressifs

---

## üéØ R√âSUM√â

‚úÖ **Recherche hybride impl√©ment√©e**: DB + Scraping en temps r√©el  
‚úÖ **D√©duplication automatique**: Pas de doublons  
‚úÖ **Persistance**: Offres sauvegard√©es pour plus tard  
‚úÖ **D√©sactivable**: `enable_scraping=false` pour DB uniquement  

‚ö†Ô∏è **Limitations**: Synchrone (10-30s d'attente), pas de feedback progressif  

üöÄ **Prochaine √©tape**: Fixer Celery pour avoir le mode async avec polling

---

**Date**: 2026-01-31  
**Fichiers modifi√©s**:
- `backend/app/services/search_service.py` (ajout `search_hybrid()`)
- `backend/app/api/job_offer.py` (endpoint modifi√©)
